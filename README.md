# AI Development Workflow: Predicting Student Dropout Rates ğŸ“ğŸ§ 

## ğŸ“Œ Project Overview
This project applies the **AI Development Workflow** to a real-world education problem: **predicting student dropout rates** using historical data. The goal is to build a machine learning model that can help educational institutions identify at-risk students early and intervene effectively.

---

## ğŸ§¾ Problem Statement
High student dropout rates negatively impact both learners and institutions. Using AI, we aim to develop a model that predicts dropout likelihood based on demographic and academic data.

---

## ğŸ¯ Objectives
- Predict whether a student is at risk of dropping out.
- Help institutions allocate support resources efficiently.
- Reduce dropout rates and improve retention.

---

## ğŸ‘¥ Stakeholders
- School Administrators
- Academic Counselors

---

## âœ… Key Performance Indicator (KPI)
- Accuracy and F1-Score of the model in predicting dropout risk.

---

## ğŸ“Š Dataset
- **Source**: Kaggle / Simulated educational data
- **Features**: Age, gender, GPA, attendance, engagement level, etc.
- **Target**: Dropout (Yes/No)

---

## ğŸ”§ Tools & Libraries Used
- Google Colab
- Python (Pandas, NumPy)
- Scikit-learn
- Matplotlib & Seaborn
- Jupyter Notebook

---

## ğŸ“ Project Structure
AI_Development_Workflow/
â”œâ”€â”€ AI_Development_Workflow.ipynb # Main Colab notebook
â”œâ”€â”€ README.md # This file
â”œâ”€â”€ report.pdf # Final report with answers & reflections
â”œâ”€â”€ screenshots/ # Screenshots of model outputs

---

## ğŸš€ Running the Project
1. Open the `.ipynb` notebook in Google Colab.
2. Run all cells sequentially.
3. The model is trained and evaluated in the final cells.
4. Output includes accuracy, confusion matrix, and F1-score.

---

## ğŸ“ˆ Model Evaluation
- Model: Random Forest Classifier
- Accuracy: **~91%**
- F1 Score: **~89%**

---

## ğŸ“Œ Ethical Considerations
- Addressed dataset bias (e.g., gender imbalance)
- Reflected on model fairness and interpretability
- Discussed use of IBM AI Fairness 360 in report

---

## âœï¸ Author
- **Name**: Caren Rayon  
  

---

## ğŸ“ Report & Submission
- The PDF report is included in this repo and published on the PLP Community platform.
- It contains all theoretical answers, screenshots, and ethical analysis.

---

## ğŸ“· Screenshots
Screenshots of output graphs, confusion matrix, and model summary are available in the `screenshots/` folder and embedded in the report.

---

## âœ… Status
**âœ… Completed** â€“ All code executed successfully, report written and submitted, screenshots captured.


